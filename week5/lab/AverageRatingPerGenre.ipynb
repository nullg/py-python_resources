{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMP5349 Week 5 Spark Lab Sample Code\n",
    "\n",
    "This is a sample notebook showing basic spark RDD operations.\n",
    "The program has two input data sources: *ratings.csv* and *movies.csv*\n",
    "The *movies.csv* file contains movie information. Each  row represents one movie, and has the following format:\n",
    "```\n",
    "movieId,title,genres\n",
    "```\n",
    "\n",
    "The *ratings.csv* file contains rating information. Each row represents one rating of one movie by one user, and has the following format:\n",
    "\n",
    "```\n",
    "userId,movieId,rating,timestamp\n",
    "```\n",
    "\n",
    "Spark can get data from various data source. The example obatin data from an external cluster and write the output on your own HDFS.\n",
    "\n",
    "Click *run cell* button on the menu to run the following two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from ml_utils import *\n",
    "findspark.init()\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"Average Rating per Genre\")\n",
    "\n",
    "#You can change the input path pointing to your own HDFS\n",
    "#If spark is able to read hadoop configuration, you can use relative path\n",
    "input_path = 'hdfs://soit-hdp-pro-1.ucc.usyd.edu.au/share/movie/small/'\n",
    "\n",
    "#Relative path is used to specify the output directory\n",
    "#The relative path is always relative to your home directory in HDFS: /user/<yourUserName>\n",
    "output_path = 'ratingOut'\n",
    "\n",
    "ratings = sc.textFile(input_path + \"ratings.csv\")\n",
    "movieData = sc.textFile(input_path + \"movies.csv\")\n",
    "\n",
    "movieRatings = ratings.map(extractRating)\n",
    "movieGenre = movieData.flatMap(pairMovieToGenre) # we use flatMap as there are multiple genre per movie\n",
    "\n",
    "genreRatings = movieGenre.join(movieRatings).values()\n",
    "genreRatingsAverage = genreRatings.aggregateByKey((0.0,0), mergeRating, mergeCombiners, 1).map(mapAverageRating)\n",
    "\n",
    "genreRatingsAverage.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to expect\n",
    "If successful, you will not see any output. The output is written toHDFS. use ```hdfs dfs -cat ratingOut/part-00000``` on command line to read the content of the output. Similar to MapReduce, Spark named its output as ```part-xxxxx```\n",
    "\n",
    "If not successful, some error message will print out as output. The most likely cause would be input file does not exists or output path already exists. Spark also creates a new output directory if you need to write output. Remember to remove the existing directory or change the output_path name for multiple run.\n",
    "\n",
    "**Remember to run the following cell to close the sparkcontext whethre spark program runs without or with any error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#always run this to close the sparkcontext whether you spark program runs without or with any error.\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
